{
  "cells": [
    {
      "cell_type": "code",
      "id": "gAhePAuTzdj0baCtL7qiN08r",
      "metadata": {
        "tags": [],
        "id": "gAhePAuTzdj0baCtL7qiN08r"
      },
      "source": [
        "!pip install --upgrade google-cloud-bigquery pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary Libraries to import\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import base64"
      ],
      "metadata": {
        "id": "a9IWK27gAtz4"
      },
      "id": "a9IWK27gAtz4",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = \"qwiklabs-gcp-00-171b5867e51b\"\n",
        "client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "k2JOSJqdA0Pp"
      },
      "id": "k2JOSJqdA0Pp",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_model():\n",
        "  query = \"\"\"\n",
        "    CREATE OR REPLACE MODEL `AuroraFAQ.Embeddings`\n",
        "    REMOTE WITH CONNECTION `us.aurora-bay-connection`\n",
        "    OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "  \"\"\"\n",
        "  result = client.query(query)\n",
        "  print(f\"Embedding model created {result}\")"
      ],
      "metadata": {
        "id": "598PiQEMIEbP"
      },
      "id": "598PiQEMIEbP",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_faq_data():\n",
        "  query = \"\"\"\n",
        "    LOAD DATA OVERWRITE AuroraFAQ.faqs\n",
        "    (\n",
        "        question STRING,\n",
        "        answer STRING\n",
        "    )\n",
        "    FROM FILES (\n",
        "        format = 'CSV',\n",
        "        uris = ['gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv']\n",
        "    );\n",
        "  \"\"\"\n",
        "  result = client.query(query)\n",
        "  print(f\"Data loaded {result}\")"
      ],
      "metadata": {
        "id": "xVgovtnZJBKy"
      },
      "id": "xVgovtnZJBKy",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings():\n",
        "  query = \"\"\"\n",
        "    CREATE OR REPLACE TABLE `AuroraFAQ.faqs_embeddings` AS\n",
        "    SELECT\n",
        "        *\n",
        "    FROM\n",
        "        ML.GENERATE_TEXT_EMBEDDING(\n",
        "            MODEL `AuroraFAQ.Embeddings`,\n",
        "            (SELECT\n",
        "                question,\n",
        "                answer,\n",
        "                CONCAT(question, ': ', answer) AS content\n",
        "            FROM\n",
        "                `AuroraFAQ.faqs`\n",
        "            )\n",
        "        ) as e;\n",
        "  \"\"\"\n",
        "  result = client.query(query)\n",
        "  print(f\"Embeddings created {result}\")"
      ],
      "metadata": {
        "id": "dILApziEJu5A"
      },
      "id": "dILApziEJu5A",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vector search query to find the user query against embedding table\n",
        "def vector_search_query(user_query, top_k):\n",
        "  return f\"\"\"\n",
        "  SELECT query.query, base.content, base.question, base.answer\n",
        "  FROM VECTOR_SEARCH(\n",
        "    TABLE `AuroraFAQ.faqs_embeddings`,\n",
        "    'text_embedding',\n",
        "    (\n",
        "      SELECT ml_generate_embedding_result, content AS query\n",
        "      FROM ML.GENERATE_EMBEDDING(\n",
        "        MODEL `AuroraFAQ.Embeddings`,\n",
        "        (SELECT '{user_query}' AS content)\n",
        "      )\n",
        "    ),\n",
        "    top_k => {top_k},\n",
        "    options => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "  ) AS search_result\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "Nue_Ij0HA2Ex"
      },
      "id": "Nue_Ij0HA2Ex",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gen AI Client\n",
        "genai_client = genai.Client(\n",
        "      vertexai=True,\n",
        "      project=\"qwiklabs-gcp-00-171b5867e51b\",\n",
        "      location=\"global\",\n",
        ")"
      ],
      "metadata": {
        "id": "vc5tIRMsDovS"
      },
      "id": "vc5tIRMsDovS",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gen AI Model\n",
        "model = \"gemini-2.5-pro-preview-06-05\""
      ],
      "metadata": {
        "id": "6zWjIBVsD0Aj"
      },
      "id": "6zWjIBVsD0Aj",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gen AI Content generation function\n",
        "def generate(system_prompt, user_input):\n",
        "  contents = [\n",
        "    types.Content(\n",
        "      role=\"user\",\n",
        "      parts=[\n",
        "        types.Part.from_text(text=user_input)\n",
        "      ]\n",
        "    ),\n",
        "  ]\n",
        "\n",
        "  generate_content_config = types.GenerateContentConfig(\n",
        "    temperature = 0.7, # Control the content creativity\n",
        "    top_p = 1, #control the probability of token selection\n",
        "    max_output_tokens = 65535,\n",
        "    system_instruction=[types.Part.from_text(text=system_prompt)],\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      thinking_budget=-1,\n",
        "    ),\n",
        "  )\n",
        "\n",
        "  for chunk in genai_client.models.generate_content_stream(\n",
        "    model = model,\n",
        "    contents = contents,\n",
        "    config = generate_content_config,\n",
        "    ):\n",
        "    print(chunk.text, end=\"\")"
      ],
      "metadata": {
        "id": "fwZ0MZdaDmH4"
      },
      "id": "fwZ0MZdaDmH4",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faq_chatbot(user_input):\n",
        "  query = vector_search_query(user_input, 5) #generating vector search query\n",
        "  query_job = client.query(query) # executing vector search query\n",
        "  df = query_job.to_dataframe() # converting search result to dataframe\n",
        "\n",
        "  if df.empty:\n",
        "      # when no matching result found in DB\n",
        "      print(\"I can't help with this.\")\n",
        "  else:\n",
        "    system_prompt = \"\"\"You are an expert FAQ assistant. Based on the following FAQs, provide the most relevant and human-like answer to the user's question.\n",
        "If none of the answers are relevant, respond with 'I can't help with this.'\\n\\n\"\"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        system_prompt += f\"FAQ {idx+1}:\\nQ: {row['question']}\\nA: {row['answer']}\\n\\n\"\n",
        "\n",
        "    # Sending the searched result to Gemini to get proper humanlike response\n",
        "    generate(system_prompt, user_input)\n",
        ""
      ],
      "metadata": {
        "id": "nEuwqtYeEC_s"
      },
      "id": "nEuwqtYeEC_s",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Embedding Model\n",
        "create_embedding_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq6AumOHKNsD",
        "outputId": "fe1351bf-01cf-4ee7-84a7-7cfdd7b4493b"
      },
      "id": "Jq6AumOHKNsD",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model created QueryJob<project=qwiklabs-gcp-00-171b5867e51b, location=US, id=826d454b-7120-46a3-b627-f55e47bdf806>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the FAQ data from Google Storage to Table\n",
        "load_faq_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPqq-RHiKD8F",
        "outputId": "04b160e2-1576-4bf8-d88d-a90eb102124d"
      },
      "id": "KPqq-RHiKD8F",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded QueryJob<project=qwiklabs-gcp-00-171b5867e51b, location=US, id=f62b5b7a-5866-4748-af91-56b61911bba7>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating embeddings for the faq table's question & answer\n",
        "create_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpL4QgEHJ5Kn",
        "outputId": "65447b89-c1bb-4e55-f949-e8aa6a50e65f"
      },
      "id": "kpL4QgEHJ5Kn",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings created QueryJob<project=qwiklabs-gcp-00-171b5867e51b, location=US, id=2709d85f-9cf7-4d49-8cb7-2bab312e31ac>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1:\n",
        "faq_chatbot(\"aurora bay population?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd7fhXT8GPw4",
        "outputId": "bc79d23e-a67d-45d3-be5a-039e095a4fdc"
      },
      "id": "Bd7fhXT8GPw4",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aurora Bay has a population of approximately 3,200 residents, although it can fluctuate seasonally due to temporary fishing and tourism workforces."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2:\n",
        "faq_chatbot(\"who is U.S president?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqfNxRyBGd4h",
        "outputId": "ddac95bf-7ae0-4603-d5ea-f0717db30752"
      },
      "id": "fqfNxRyBGd4h",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can't help with this."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-03-8fac8ec349ce (Jun 16, 2025, 12:12:49â€¯PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}